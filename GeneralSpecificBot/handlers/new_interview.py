import re
import pymorphy2
from aiogram import Dispatcher, F
from aiogram.fsm.context import FSMContext
from aiogram.types import Message, Document
from docx import Document as DocxDocument
from tempfile import NamedTemporaryFile
from fsm import InterviewStates
from transformers import pipeline

morph = pymorphy2.MorphAnalyzer()

# –ø–µ—Ä–≤–æ–µ –ª–∏—Ü–æ –≤–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —á–∏—Å–ª–µ (–º—ã, –Ω–∞—à)
FIRST_PERSON_PLURAL_LEMMAS = {"–º—ã", "–Ω–∞—à"}

# –ø–µ—Ä–≤–æ–µ –ª–∏—Ü–æ –≤ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–º —á–∏—Å–ª–µ (—è, –º–æ–π)
FIRST_PERSON_SINGULAR_LEMMAS = {"—è", "–º–æ–π"}

# —Å—É—Ñ—Ñ–∏–∫—Å—ã, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–µ –¥–ª—è –æ—Ç–≥–ª–∞–≥–æ–ª—å–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
DEVERBAL_SUFFIXES = ("–Ω–∏–µ", "—Ü–∏—è", "—Ç–µ–ª—å—Å—Ç–≤–æ", "–∞–Ω–∏–µ", "–µ–Ω–∏–µ", "–∫–∞")

# –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ —Å–ª–æ–≤–∞
def get_lemma(word: str) -> str:
    return morph.parse(word)[0].normal_form

# –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –æ—Ç–≥–ª–∞–≥–æ–ª—å–Ω–æ–µ
def looks_like_deverbal(noun: str) -> bool:
    return noun.endswith(DEVERBAL_SUFFIXES)

# –ø–æ–¥—Å—á–µ—Ç –û–ë–©–ò–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def contains_label0_features(sentence: str) -> int:
    words = sentence.lower().split()
    counter = 0
    for word in words:
        parsed = morph.parse(word)[0]
        lemma = parsed.normal_form
        tag = parsed.tag

        # –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ 1-–≥–æ –ª–∏—Ü–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞
        if lemma in FIRST_PERSON_PLURAL_LEMMAS:
            counter += 1

        # –≥–ª–∞–≥–æ–ª 1-–≥–æ –ª–∏—Ü–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞
        if ("VERB" in tag or "INFN" in tag) and "1per" in tag and "plur" in tag:
            counter += 1

        # –≥–ª–∞–≥–æ–ª —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞
        if ("VERB" in tag or "INFN" in tag) and "perf" in tag:
            counter += 1

        # –ø–∞—Å—Å–∏–≤–Ω–∞—è —Ñ–æ—Ä–º–∞
        if "pssv" in tag:
            counter += 1

        # –ø—Ä–∏—á–∞—Å—Ç–∏–µ –∏–ª–∏ –¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏–µ
        if "PRTF" in tag or "GRND" in tag:
            counter += 1

    return counter

# –ø–æ–¥—Å—á–µ—Ç –ß–ê–°–¢–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def contains_label1_features(sentence: str) -> int:
    words = sentence.lower().split()
    parsed_words = [(w, morph.parse(w)[0]) for w in words]
    counter = 0

    for w, p in parsed_words:
        lemma = p.normal_form
        tag = p.tag

        # –º–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ 1-–≥–æ –ª–∏—Ü–∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞
        if lemma in FIRST_PERSON_SINGULAR_LEMMAS:
            counter += 1

        # –≥–ª–∞–≥–æ–ª 1-–≥–æ –ª–∏—Ü–∞ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∏—Å–ª–∞
        if ("VERB" in tag or "INFN" in tag) and "1per" in tag and "sing" in tag:
            counter += 1

    # –æ—Ç–≥–ª–∞–≥–æ–ª—å–Ω–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
    if any("NOUN" in p.tag and looks_like_deverbal(w) for w, p in parsed_words):
        counter += 1

    # –≥–ª–∞–≥–æ–ª—ã –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–∞
    if any(("VERB" in p.tag or "INFN" in p.tag) and "impf" in p.tag for w, p in parsed_words):
        counter += 1

    # —Ç—Ä–∏ –∏ –±–æ–ª–µ–µ –≥–ª–∞–≥–æ–ª–∞ –ø–æ–¥—Ä—è–¥
    tokens = re.split(r"[,\s]+", sentence.lower())
    run = 0
    for t in tokens:
        p = morph.parse(t)[0]
        if "VERB" in p.tag or "INFN" in p.tag:
            run += 1
            if run >= 3:
                counter += 1
        else:
            run = 0

    return counter

analyzer_pipeline = pipeline("text-classification", model="ekas03/my_genspec_analyzer")

def split_into_sentences(text: str):
    return re.split(r'(?<=[.!?])\s+', text.strip())

def new_interview(dp: Dispatcher):
    @dp.message(F.text == "/new_interview")
    async def message_interview(message: Message, state: FSMContext):
        await message.answer(
            "–û—Ç–ø—Ä–∞–≤—å —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ .DOCX —Å –∏–Ω—Ç–µ—Ä–≤—å—é, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å \"–í:\", –∞ –æ—Ç–≤–µ—Ç—ã —Å \"–û:\""
        )
        await state.set_state(InterviewStates.waiting_for_docx)

    @dp.message(InterviewStates.waiting_for_docx, F.document)
    async def handle_docx(message: Message, state: FSMContext):
        doc: Document = message.document

        if not doc.file_name.lower().endswith(".docx"):
            await message.answer("–û–®–ò–ë–ö–ê: –Ω—É–∂–µ–Ω —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ .DOCX —Å –∏–Ω—Ç–µ—Ä–≤—å—é, –≥–¥–µ –≤–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å \"–í:\", –∞ –æ—Ç–≤–µ—Ç—ã —Å \"–û:\"! –û—Ç–ø—Ä–∞–≤—å –Ω–æ–≤–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é:")
            return

        try:
            file = await message.bot.get_file(doc.file_id)
            file_bytes = await message.bot.download_file(file.file_path)

            with NamedTemporaryFile(suffix=".docx", delete=False) as tmp:
                tmp.write(file_bytes.read())
                tmp_path = tmp.name

            parsed = DocxDocument(tmp_path)
            lines = [p.text.strip() for p in parsed.paragraphs if p.text.strip()]
            valid = all(line.startswith("–í:") or line.startswith("–û:") for line in lines)

            if not valid:
                await message.answer(
                    "–û–®–ò–ë–ö–ê: –Ω–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–Ω—Ç–µ—Ä–≤—å—é!\n"
                    "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –≤–æ–ø—Ä–æ—Å—ã –Ω–∞—á–∏–Ω–∞–ª–∏—Å—å —Å \"–í:\", –∞ –æ—Ç–≤–µ—Ç—ã —Å \"–û:\".\n"
                    "–û—Ç–ø—Ä–∞–≤—å –Ω–æ–≤–æ–µ –∏–Ω—Ç–µ—Ä–≤—å—é:"
                )
                return

            sentences = []
            for line in lines:
                if line.startswith("–û:"):
                    content = line[2:].strip()
                    sentences.extend(split_into_sentences(content))

            if not sentences:
                await message.answer("–û–®–ò–ë–ö–ê: –æ—Ç–≤–µ—Ç–æ–≤ (–Ω–∞—á–∏–Ω–∞—é—Ç—Å—è —Å \"–û:\") –Ω–µ –Ω–∞–π–¥–µ–Ω—ã! –ü–æ–ø—Ä–æ–±—É–π —Å–Ω–æ–≤–∞ /new_interview")
                return

            counts_model = {"LABEL_0": 0, "LABEL_1": 0}
            counts_morph = {"LABEL_0": 0, "LABEL_1": 0}

            for sent in sentences:
                model_result = analyzer_pipeline(sent, truncation=True)[0]
                counts_model[model_result["label"]] += 1

                score0 = contains_label0_features(sent)
                score1 = contains_label1_features(sent)
                morph_label = "LABEL_0" if score0 > score1 else "LABEL_1"
                counts_morph[morph_label] += 1

            classification_model = "–û–ë–©–ï–ï" if counts_model["LABEL_0"] > counts_model["LABEL_1"] else "–ß–ê–°–¢–ù–û–ï"
            classification_morph = "–û–ë–©–ï–ï" if counts_morph["LABEL_0"] > counts_morph["LABEL_1"] else "–ß–ê–°–¢–ù–û–ï"

            await message.answer(
                "üìä –ò–Ω—Ç–µ—Ä–≤—å—é –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:\n\n"
                f"<b>–ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏</b>:\n"
                f"–û–ë–©–ò–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {counts_model['LABEL_0']}\n"
                f"–ß–ê–°–¢–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {counts_model['LABEL_1']}\n"
                f"<b>–†–µ–∑—É–ª—å—Ç–∞—Ç:</b> {classification_model}\n\n"
                f"<b>–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑</b>:\n"
                f"–û–ë–©–ò–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {counts_morph['LABEL_0']}\n"
                f"–ß–ê–°–¢–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {counts_morph['LABEL_1']}\n"
                f"<b>–†–µ–∑—É–ª—å—Ç–∞—Ç:</b> {classification_morph}"
            )
            await state.clear()

        except Exception as e:
            print(f"[Analyzer error] {e}")
            await message.answer("–ü—Ä–æ–∏—Ö–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –ø–æ–ø—Ä–æ–±—É–π —Å–Ω–æ–≤–∞ /new_interview")
